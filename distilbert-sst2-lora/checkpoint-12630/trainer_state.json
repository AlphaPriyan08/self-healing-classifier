{
  "best_global_step": 4210,
  "best_metric": 0.27403780817985535,
  "best_model_checkpoint": "distilbert-sst2-lora\\checkpoint-4210",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 12630,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011876484560570071,
      "grad_norm": 2.0789132118225098,
      "learning_rate": 0.0001992240696753761,
      "loss": 0.575,
      "step": 50
    },
    {
      "epoch": 0.023752969121140142,
      "grad_norm": 1.506070613861084,
      "learning_rate": 0.00019843230403800477,
      "loss": 0.4028,
      "step": 100
    },
    {
      "epoch": 0.035629453681710214,
      "grad_norm": 4.00647497177124,
      "learning_rate": 0.00019764053840063341,
      "loss": 0.3433,
      "step": 150
    },
    {
      "epoch": 0.047505938242280284,
      "grad_norm": 1.142580509185791,
      "learning_rate": 0.00019684877276326208,
      "loss": 0.3617,
      "step": 200
    },
    {
      "epoch": 0.05938242280285035,
      "grad_norm": 0.8763335943222046,
      "learning_rate": 0.00019605700712589075,
      "loss": 0.2898,
      "step": 250
    },
    {
      "epoch": 0.07125890736342043,
      "grad_norm": 1.5043129920959473,
      "learning_rate": 0.00019526524148851942,
      "loss": 0.3253,
      "step": 300
    },
    {
      "epoch": 0.0831353919239905,
      "grad_norm": 2.090599298477173,
      "learning_rate": 0.00019447347585114808,
      "loss": 0.3467,
      "step": 350
    },
    {
      "epoch": 0.09501187648456057,
      "grad_norm": 2.0756702423095703,
      "learning_rate": 0.00019368171021377672,
      "loss": 0.3057,
      "step": 400
    },
    {
      "epoch": 0.10688836104513064,
      "grad_norm": 2.1439194679260254,
      "learning_rate": 0.0001928899445764054,
      "loss": 0.3046,
      "step": 450
    },
    {
      "epoch": 0.1187648456057007,
      "grad_norm": 2.2168474197387695,
      "learning_rate": 0.00019209817893903406,
      "loss": 0.334,
      "step": 500
    },
    {
      "epoch": 0.13064133016627077,
      "grad_norm": 1.8927968740463257,
      "learning_rate": 0.00019130641330166272,
      "loss": 0.3056,
      "step": 550
    },
    {
      "epoch": 0.14251781472684086,
      "grad_norm": 0.8017910718917847,
      "learning_rate": 0.00019051464766429136,
      "loss": 0.3485,
      "step": 600
    },
    {
      "epoch": 0.1543942992874109,
      "grad_norm": 1.2511789798736572,
      "learning_rate": 0.00018972288202692003,
      "loss": 0.2924,
      "step": 650
    },
    {
      "epoch": 0.166270783847981,
      "grad_norm": 2.3147518634796143,
      "learning_rate": 0.0001889311163895487,
      "loss": 0.275,
      "step": 700
    },
    {
      "epoch": 0.17814726840855108,
      "grad_norm": 1.2677932977676392,
      "learning_rate": 0.00018813935075217737,
      "loss": 0.2973,
      "step": 750
    },
    {
      "epoch": 0.19002375296912113,
      "grad_norm": 1.6998398303985596,
      "learning_rate": 0.00018734758511480603,
      "loss": 0.2981,
      "step": 800
    },
    {
      "epoch": 0.20190023752969122,
      "grad_norm": 3.4563913345336914,
      "learning_rate": 0.00018655581947743467,
      "loss": 0.3119,
      "step": 850
    },
    {
      "epoch": 0.21377672209026127,
      "grad_norm": 1.5507726669311523,
      "learning_rate": 0.00018576405384006334,
      "loss": 0.3317,
      "step": 900
    },
    {
      "epoch": 0.22565320665083136,
      "grad_norm": 2.58581805229187,
      "learning_rate": 0.000184972288202692,
      "loss": 0.2998,
      "step": 950
    },
    {
      "epoch": 0.2375296912114014,
      "grad_norm": 4.942234516143799,
      "learning_rate": 0.00018418052256532067,
      "loss": 0.2763,
      "step": 1000
    },
    {
      "epoch": 0.2494061757719715,
      "grad_norm": 2.2607321739196777,
      "learning_rate": 0.00018338875692794934,
      "loss": 0.2928,
      "step": 1050
    },
    {
      "epoch": 0.26128266033254155,
      "grad_norm": 1.2353094816207886,
      "learning_rate": 0.00018259699129057798,
      "loss": 0.2994,
      "step": 1100
    },
    {
      "epoch": 0.27315914489311166,
      "grad_norm": 1.1977711915969849,
      "learning_rate": 0.00018180522565320665,
      "loss": 0.2677,
      "step": 1150
    },
    {
      "epoch": 0.2850356294536817,
      "grad_norm": 5.0278120040893555,
      "learning_rate": 0.00018101346001583532,
      "loss": 0.236,
      "step": 1200
    },
    {
      "epoch": 0.29691211401425177,
      "grad_norm": 1.826156735420227,
      "learning_rate": 0.00018022169437846398,
      "loss": 0.303,
      "step": 1250
    },
    {
      "epoch": 0.3087885985748218,
      "grad_norm": 1.840346097946167,
      "learning_rate": 0.00017942992874109262,
      "loss": 0.2763,
      "step": 1300
    },
    {
      "epoch": 0.32066508313539194,
      "grad_norm": 1.1267017126083374,
      "learning_rate": 0.0001786381631037213,
      "loss": 0.2826,
      "step": 1350
    },
    {
      "epoch": 0.332541567695962,
      "grad_norm": 1.845750331878662,
      "learning_rate": 0.00017784639746634996,
      "loss": 0.2577,
      "step": 1400
    },
    {
      "epoch": 0.34441805225653205,
      "grad_norm": 0.6228330135345459,
      "learning_rate": 0.00017705463182897862,
      "loss": 0.2423,
      "step": 1450
    },
    {
      "epoch": 0.35629453681710216,
      "grad_norm": 1.4974313974380493,
      "learning_rate": 0.0001762628661916073,
      "loss": 0.301,
      "step": 1500
    },
    {
      "epoch": 0.3681710213776722,
      "grad_norm": 1.3545198440551758,
      "learning_rate": 0.00017547110055423593,
      "loss": 0.2867,
      "step": 1550
    },
    {
      "epoch": 0.38004750593824227,
      "grad_norm": 2.22831130027771,
      "learning_rate": 0.0001746793349168646,
      "loss": 0.2286,
      "step": 1600
    },
    {
      "epoch": 0.3919239904988123,
      "grad_norm": 1.5431355237960815,
      "learning_rate": 0.00017388756927949327,
      "loss": 0.2877,
      "step": 1650
    },
    {
      "epoch": 0.40380047505938244,
      "grad_norm": 2.4080300331115723,
      "learning_rate": 0.00017309580364212193,
      "loss": 0.2528,
      "step": 1700
    },
    {
      "epoch": 0.4156769596199525,
      "grad_norm": 1.6197391748428345,
      "learning_rate": 0.0001723040380047506,
      "loss": 0.2638,
      "step": 1750
    },
    {
      "epoch": 0.42755344418052255,
      "grad_norm": 3.1920382976531982,
      "learning_rate": 0.00017151227236737927,
      "loss": 0.2666,
      "step": 1800
    },
    {
      "epoch": 0.43942992874109266,
      "grad_norm": 2.406992197036743,
      "learning_rate": 0.00017072050673000794,
      "loss": 0.2716,
      "step": 1850
    },
    {
      "epoch": 0.4513064133016627,
      "grad_norm": 1.3092364072799683,
      "learning_rate": 0.0001699287410926366,
      "loss": 0.2823,
      "step": 1900
    },
    {
      "epoch": 0.46318289786223277,
      "grad_norm": 4.0038886070251465,
      "learning_rate": 0.00016913697545526527,
      "loss": 0.2879,
      "step": 1950
    },
    {
      "epoch": 0.4750593824228028,
      "grad_norm": 1.9279210567474365,
      "learning_rate": 0.0001683452098178939,
      "loss": 0.2531,
      "step": 2000
    },
    {
      "epoch": 0.48693586698337293,
      "grad_norm": 1.1369348764419556,
      "learning_rate": 0.00016755344418052258,
      "loss": 0.3297,
      "step": 2050
    },
    {
      "epoch": 0.498812351543943,
      "grad_norm": 1.9424798488616943,
      "learning_rate": 0.00016676167854315124,
      "loss": 0.2621,
      "step": 2100
    },
    {
      "epoch": 0.5106888361045131,
      "grad_norm": 2.0925564765930176,
      "learning_rate": 0.0001659699129057799,
      "loss": 0.2298,
      "step": 2150
    },
    {
      "epoch": 0.5225653206650831,
      "grad_norm": 1.1293249130249023,
      "learning_rate": 0.00016517814726840858,
      "loss": 0.2793,
      "step": 2200
    },
    {
      "epoch": 0.5344418052256532,
      "grad_norm": 1.7441091537475586,
      "learning_rate": 0.00016438638163103722,
      "loss": 0.2163,
      "step": 2250
    },
    {
      "epoch": 0.5463182897862233,
      "grad_norm": 2.3722784519195557,
      "learning_rate": 0.00016359461599366589,
      "loss": 0.2382,
      "step": 2300
    },
    {
      "epoch": 0.5581947743467933,
      "grad_norm": 1.2040841579437256,
      "learning_rate": 0.00016280285035629455,
      "loss": 0.2477,
      "step": 2350
    },
    {
      "epoch": 0.5700712589073634,
      "grad_norm": 1.0639914274215698,
      "learning_rate": 0.00016201108471892322,
      "loss": 0.2137,
      "step": 2400
    },
    {
      "epoch": 0.5819477434679335,
      "grad_norm": 1.1500310897827148,
      "learning_rate": 0.0001612193190815519,
      "loss": 0.2543,
      "step": 2450
    },
    {
      "epoch": 0.5938242280285035,
      "grad_norm": 1.4651856422424316,
      "learning_rate": 0.00016042755344418053,
      "loss": 0.2567,
      "step": 2500
    },
    {
      "epoch": 0.6057007125890737,
      "grad_norm": 2.306344985961914,
      "learning_rate": 0.0001596357878068092,
      "loss": 0.2421,
      "step": 2550
    },
    {
      "epoch": 0.6175771971496437,
      "grad_norm": 2.778700351715088,
      "learning_rate": 0.00015884402216943786,
      "loss": 0.271,
      "step": 2600
    },
    {
      "epoch": 0.6294536817102138,
      "grad_norm": 0.8332370519638062,
      "learning_rate": 0.00015805225653206653,
      "loss": 0.295,
      "step": 2650
    },
    {
      "epoch": 0.6413301662707839,
      "grad_norm": 1.4617263078689575,
      "learning_rate": 0.00015726049089469517,
      "loss": 0.2417,
      "step": 2700
    },
    {
      "epoch": 0.6532066508313539,
      "grad_norm": 1.574137806892395,
      "learning_rate": 0.00015646872525732384,
      "loss": 0.2709,
      "step": 2750
    },
    {
      "epoch": 0.665083135391924,
      "grad_norm": 2.2444822788238525,
      "learning_rate": 0.0001556769596199525,
      "loss": 0.2357,
      "step": 2800
    },
    {
      "epoch": 0.6769596199524941,
      "grad_norm": 1.544959306716919,
      "learning_rate": 0.00015488519398258117,
      "loss": 0.2734,
      "step": 2850
    },
    {
      "epoch": 0.6888361045130641,
      "grad_norm": 1.950866460800171,
      "learning_rate": 0.00015409342834520984,
      "loss": 0.2532,
      "step": 2900
    },
    {
      "epoch": 0.7007125890736342,
      "grad_norm": 2.9178309440612793,
      "learning_rate": 0.00015330166270783848,
      "loss": 0.2615,
      "step": 2950
    },
    {
      "epoch": 0.7125890736342043,
      "grad_norm": 1.1784064769744873,
      "learning_rate": 0.00015250989707046714,
      "loss": 0.2636,
      "step": 3000
    },
    {
      "epoch": 0.7244655581947743,
      "grad_norm": 2.671694755554199,
      "learning_rate": 0.0001517181314330958,
      "loss": 0.2767,
      "step": 3050
    },
    {
      "epoch": 0.7363420427553444,
      "grad_norm": 4.187505722045898,
      "learning_rate": 0.00015092636579572448,
      "loss": 0.249,
      "step": 3100
    },
    {
      "epoch": 0.7482185273159145,
      "grad_norm": 1.9334135055541992,
      "learning_rate": 0.00015013460015835315,
      "loss": 0.2649,
      "step": 3150
    },
    {
      "epoch": 0.7600950118764845,
      "grad_norm": 0.8619474768638611,
      "learning_rate": 0.00014934283452098179,
      "loss": 0.2777,
      "step": 3200
    },
    {
      "epoch": 0.7719714964370546,
      "grad_norm": 2.5097100734710693,
      "learning_rate": 0.00014855106888361045,
      "loss": 0.2224,
      "step": 3250
    },
    {
      "epoch": 0.7838479809976246,
      "grad_norm": 3.262389659881592,
      "learning_rate": 0.00014775930324623912,
      "loss": 0.2221,
      "step": 3300
    },
    {
      "epoch": 0.7957244655581948,
      "grad_norm": 3.875518321990967,
      "learning_rate": 0.0001469675376088678,
      "loss": 0.2391,
      "step": 3350
    },
    {
      "epoch": 0.8076009501187649,
      "grad_norm": 1.7767045497894287,
      "learning_rate": 0.00014617577197149643,
      "loss": 0.2674,
      "step": 3400
    },
    {
      "epoch": 0.8194774346793349,
      "grad_norm": 0.9646267294883728,
      "learning_rate": 0.0001453840063341251,
      "loss": 0.269,
      "step": 3450
    },
    {
      "epoch": 0.831353919239905,
      "grad_norm": 1.4127869606018066,
      "learning_rate": 0.00014459224069675376,
      "loss": 0.2448,
      "step": 3500
    },
    {
      "epoch": 0.8432304038004751,
      "grad_norm": 1.7357476949691772,
      "learning_rate": 0.00014380047505938243,
      "loss": 0.2577,
      "step": 3550
    },
    {
      "epoch": 0.8551068883610451,
      "grad_norm": 1.851696252822876,
      "learning_rate": 0.0001430087094220111,
      "loss": 0.2505,
      "step": 3600
    },
    {
      "epoch": 0.8669833729216152,
      "grad_norm": 1.0916063785552979,
      "learning_rate": 0.00014221694378463974,
      "loss": 0.2435,
      "step": 3650
    },
    {
      "epoch": 0.8788598574821853,
      "grad_norm": 1.5430270433425903,
      "learning_rate": 0.0001414251781472684,
      "loss": 0.2295,
      "step": 3700
    },
    {
      "epoch": 0.8907363420427553,
      "grad_norm": 2.659501314163208,
      "learning_rate": 0.00014063341250989707,
      "loss": 0.2477,
      "step": 3750
    },
    {
      "epoch": 0.9026128266033254,
      "grad_norm": 2.2707643508911133,
      "learning_rate": 0.00013984164687252574,
      "loss": 0.2432,
      "step": 3800
    },
    {
      "epoch": 0.9144893111638955,
      "grad_norm": 2.9096362590789795,
      "learning_rate": 0.0001390498812351544,
      "loss": 0.2153,
      "step": 3850
    },
    {
      "epoch": 0.9263657957244655,
      "grad_norm": 0.9728354215621948,
      "learning_rate": 0.00013825811559778304,
      "loss": 0.2093,
      "step": 3900
    },
    {
      "epoch": 0.9382422802850356,
      "grad_norm": 2.152759552001953,
      "learning_rate": 0.0001374663499604117,
      "loss": 0.2267,
      "step": 3950
    },
    {
      "epoch": 0.9501187648456056,
      "grad_norm": 2.347852945327759,
      "learning_rate": 0.00013667458432304038,
      "loss": 0.2424,
      "step": 4000
    },
    {
      "epoch": 0.9619952494061758,
      "grad_norm": 1.4990286827087402,
      "learning_rate": 0.00013588281868566905,
      "loss": 0.2356,
      "step": 4050
    },
    {
      "epoch": 0.9738717339667459,
      "grad_norm": 1.0728973150253296,
      "learning_rate": 0.0001350910530482977,
      "loss": 0.2374,
      "step": 4100
    },
    {
      "epoch": 0.9857482185273159,
      "grad_norm": 1.2728657722473145,
      "learning_rate": 0.00013429928741092635,
      "loss": 0.2322,
      "step": 4150
    },
    {
      "epoch": 0.997624703087886,
      "grad_norm": 2.659296751022339,
      "learning_rate": 0.00013350752177355502,
      "loss": 0.2538,
      "step": 4200
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.893348623853211,
      "eval_loss": 0.27403780817985535,
      "eval_runtime": 25.0136,
      "eval_samples_per_second": 34.861,
      "eval_steps_per_second": 2.199,
      "step": 4210
    },
    {
      "epoch": 1.009501187648456,
      "grad_norm": 1.6887052059173584,
      "learning_rate": 0.0001327157561361837,
      "loss": 0.208,
      "step": 4250
    },
    {
      "epoch": 1.0213776722090262,
      "grad_norm": 4.268126010894775,
      "learning_rate": 0.00013192399049881235,
      "loss": 0.2354,
      "step": 4300
    },
    {
      "epoch": 1.0332541567695963,
      "grad_norm": 1.863988995552063,
      "learning_rate": 0.00013113222486144102,
      "loss": 0.1997,
      "step": 4350
    },
    {
      "epoch": 1.0451306413301662,
      "grad_norm": 2.748694896697998,
      "learning_rate": 0.0001303404592240697,
      "loss": 0.2361,
      "step": 4400
    },
    {
      "epoch": 1.0570071258907363,
      "grad_norm": 3.355440139770508,
      "learning_rate": 0.00012954869358669836,
      "loss": 0.222,
      "step": 4450
    },
    {
      "epoch": 1.0688836104513064,
      "grad_norm": 1.2629859447479248,
      "learning_rate": 0.00012875692794932702,
      "loss": 0.2001,
      "step": 4500
    },
    {
      "epoch": 1.0807600950118765,
      "grad_norm": 2.466874837875366,
      "learning_rate": 0.0001279651623119557,
      "loss": 0.2314,
      "step": 4550
    },
    {
      "epoch": 1.0926365795724466,
      "grad_norm": 3.813333034515381,
      "learning_rate": 0.00012717339667458433,
      "loss": 0.226,
      "step": 4600
    },
    {
      "epoch": 1.1045130641330165,
      "grad_norm": 2.916712760925293,
      "learning_rate": 0.000126381631037213,
      "loss": 0.222,
      "step": 4650
    },
    {
      "epoch": 1.1163895486935866,
      "grad_norm": 2.5931811332702637,
      "learning_rate": 0.00012558986539984166,
      "loss": 0.1771,
      "step": 4700
    },
    {
      "epoch": 1.1282660332541568,
      "grad_norm": 2.021805763244629,
      "learning_rate": 0.00012479809976247033,
      "loss": 0.236,
      "step": 4750
    },
    {
      "epoch": 1.1401425178147269,
      "grad_norm": 1.035211443901062,
      "learning_rate": 0.00012400633412509897,
      "loss": 0.2036,
      "step": 4800
    },
    {
      "epoch": 1.152019002375297,
      "grad_norm": 3.237774133682251,
      "learning_rate": 0.00012321456848772764,
      "loss": 0.2442,
      "step": 4850
    },
    {
      "epoch": 1.1638954869358669,
      "grad_norm": 1.7116384506225586,
      "learning_rate": 0.0001224228028503563,
      "loss": 0.2284,
      "step": 4900
    },
    {
      "epoch": 1.175771971496437,
      "grad_norm": 0.35444507002830505,
      "learning_rate": 0.00012163103721298496,
      "loss": 0.237,
      "step": 4950
    },
    {
      "epoch": 1.187648456057007,
      "grad_norm": 3.336719512939453,
      "learning_rate": 0.00012083927157561363,
      "loss": 0.2091,
      "step": 5000
    },
    {
      "epoch": 1.1995249406175772,
      "grad_norm": 2.134061336517334,
      "learning_rate": 0.00012004750593824228,
      "loss": 0.2244,
      "step": 5050
    },
    {
      "epoch": 1.2114014251781473,
      "grad_norm": 1.194237470626831,
      "learning_rate": 0.00011925574030087095,
      "loss": 0.2012,
      "step": 5100
    },
    {
      "epoch": 1.2232779097387174,
      "grad_norm": 1.805111289024353,
      "learning_rate": 0.00011846397466349962,
      "loss": 0.1902,
      "step": 5150
    },
    {
      "epoch": 1.2351543942992875,
      "grad_norm": 1.2676500082015991,
      "learning_rate": 0.00011767220902612828,
      "loss": 0.2399,
      "step": 5200
    },
    {
      "epoch": 1.2470308788598574,
      "grad_norm": 2.7902352809906006,
      "learning_rate": 0.00011688044338875695,
      "loss": 0.2069,
      "step": 5250
    },
    {
      "epoch": 1.2589073634204275,
      "grad_norm": 2.776174306869507,
      "learning_rate": 0.00011608867775138559,
      "loss": 0.2155,
      "step": 5300
    },
    {
      "epoch": 1.2707838479809976,
      "grad_norm": 1.9141931533813477,
      "learning_rate": 0.00011529691211401426,
      "loss": 0.1964,
      "step": 5350
    },
    {
      "epoch": 1.2826603325415677,
      "grad_norm": 2.07401704788208,
      "learning_rate": 0.00011450514647664292,
      "loss": 0.2346,
      "step": 5400
    },
    {
      "epoch": 1.2945368171021379,
      "grad_norm": 4.1846513748168945,
      "learning_rate": 0.00011371338083927159,
      "loss": 0.2155,
      "step": 5450
    },
    {
      "epoch": 1.3064133016627077,
      "grad_norm": 1.9090704917907715,
      "learning_rate": 0.00011292161520190023,
      "loss": 0.1937,
      "step": 5500
    },
    {
      "epoch": 1.3182897862232779,
      "grad_norm": 2.28106427192688,
      "learning_rate": 0.0001121298495645289,
      "loss": 0.2403,
      "step": 5550
    },
    {
      "epoch": 1.330166270783848,
      "grad_norm": 1.5405396223068237,
      "learning_rate": 0.00011133808392715757,
      "loss": 0.1784,
      "step": 5600
    },
    {
      "epoch": 1.342042755344418,
      "grad_norm": 2.1938562393188477,
      "learning_rate": 0.00011054631828978623,
      "loss": 0.2157,
      "step": 5650
    },
    {
      "epoch": 1.3539192399049882,
      "grad_norm": 3.04534912109375,
      "learning_rate": 0.0001097545526524149,
      "loss": 0.1991,
      "step": 5700
    },
    {
      "epoch": 1.365795724465558,
      "grad_norm": 0.7520281076431274,
      "learning_rate": 0.00010896278701504354,
      "loss": 0.1874,
      "step": 5750
    },
    {
      "epoch": 1.3776722090261282,
      "grad_norm": 2.4294350147247314,
      "learning_rate": 0.00010817102137767221,
      "loss": 0.175,
      "step": 5800
    },
    {
      "epoch": 1.3895486935866983,
      "grad_norm": 2.053420305252075,
      "learning_rate": 0.00010737925574030087,
      "loss": 0.1997,
      "step": 5850
    },
    {
      "epoch": 1.4014251781472684,
      "grad_norm": 2.9395666122436523,
      "learning_rate": 0.00010658749010292954,
      "loss": 0.2158,
      "step": 5900
    },
    {
      "epoch": 1.4133016627078385,
      "grad_norm": 1.19590163230896,
      "learning_rate": 0.00010579572446555821,
      "loss": 0.2138,
      "step": 5950
    },
    {
      "epoch": 1.4251781472684084,
      "grad_norm": 2.496837615966797,
      "learning_rate": 0.00010500395882818685,
      "loss": 0.2016,
      "step": 6000
    },
    {
      "epoch": 1.4370546318289787,
      "grad_norm": 0.9936047196388245,
      "learning_rate": 0.00010421219319081552,
      "loss": 0.1866,
      "step": 6050
    },
    {
      "epoch": 1.4489311163895486,
      "grad_norm": 1.283408284187317,
      "learning_rate": 0.00010342042755344418,
      "loss": 0.2348,
      "step": 6100
    },
    {
      "epoch": 1.4608076009501187,
      "grad_norm": 0.40601444244384766,
      "learning_rate": 0.00010262866191607285,
      "loss": 0.2066,
      "step": 6150
    },
    {
      "epoch": 1.4726840855106889,
      "grad_norm": 5.098232269287109,
      "learning_rate": 0.0001018368962787015,
      "loss": 0.1882,
      "step": 6200
    },
    {
      "epoch": 1.484560570071259,
      "grad_norm": 2.045429229736328,
      "learning_rate": 0.00010104513064133017,
      "loss": 0.2174,
      "step": 6250
    },
    {
      "epoch": 1.496437054631829,
      "grad_norm": 3.3194737434387207,
      "learning_rate": 0.00010025336500395884,
      "loss": 0.2156,
      "step": 6300
    },
    {
      "epoch": 1.508313539192399,
      "grad_norm": 2.5942766666412354,
      "learning_rate": 9.94615993665875e-05,
      "loss": 0.2005,
      "step": 6350
    },
    {
      "epoch": 1.520190023752969,
      "grad_norm": 7.183286190032959,
      "learning_rate": 9.866983372921616e-05,
      "loss": 0.2094,
      "step": 6400
    },
    {
      "epoch": 1.5320665083135392,
      "grad_norm": 1.3929953575134277,
      "learning_rate": 9.787806809184483e-05,
      "loss": 0.1778,
      "step": 6450
    },
    {
      "epoch": 1.5439429928741093,
      "grad_norm": 2.7600700855255127,
      "learning_rate": 9.708630245447348e-05,
      "loss": 0.238,
      "step": 6500
    },
    {
      "epoch": 1.5558194774346794,
      "grad_norm": 2.467780590057373,
      "learning_rate": 9.629453681710215e-05,
      "loss": 0.2133,
      "step": 6550
    },
    {
      "epoch": 1.5676959619952493,
      "grad_norm": 0.9412243366241455,
      "learning_rate": 9.55027711797308e-05,
      "loss": 0.1674,
      "step": 6600
    },
    {
      "epoch": 1.5795724465558196,
      "grad_norm": 1.623233437538147,
      "learning_rate": 9.471100554235947e-05,
      "loss": 0.2122,
      "step": 6650
    },
    {
      "epoch": 1.5914489311163895,
      "grad_norm": 0.48481249809265137,
      "learning_rate": 9.391923990498813e-05,
      "loss": 0.1787,
      "step": 6700
    },
    {
      "epoch": 1.6033254156769596,
      "grad_norm": 5.730860710144043,
      "learning_rate": 9.312747426761679e-05,
      "loss": 0.2037,
      "step": 6750
    },
    {
      "epoch": 1.6152019002375297,
      "grad_norm": 3.804394245147705,
      "learning_rate": 9.233570863024546e-05,
      "loss": 0.2197,
      "step": 6800
    },
    {
      "epoch": 1.6270783847980996,
      "grad_norm": 2.2673659324645996,
      "learning_rate": 9.154394299287411e-05,
      "loss": 0.2093,
      "step": 6850
    },
    {
      "epoch": 1.63895486935867,
      "grad_norm": 2.8252837657928467,
      "learning_rate": 9.075217735550278e-05,
      "loss": 0.2226,
      "step": 6900
    },
    {
      "epoch": 1.6508313539192399,
      "grad_norm": 0.45517247915267944,
      "learning_rate": 8.996041171813143e-05,
      "loss": 0.2052,
      "step": 6950
    },
    {
      "epoch": 1.66270783847981,
      "grad_norm": 1.432106852531433,
      "learning_rate": 8.91686460807601e-05,
      "loss": 0.2037,
      "step": 7000
    },
    {
      "epoch": 1.67458432304038,
      "grad_norm": 2.4109299182891846,
      "learning_rate": 8.837688044338876e-05,
      "loss": 0.1732,
      "step": 7050
    },
    {
      "epoch": 1.68646080760095,
      "grad_norm": 0.878419816493988,
      "learning_rate": 8.758511480601742e-05,
      "loss": 0.1601,
      "step": 7100
    },
    {
      "epoch": 1.6983372921615203,
      "grad_norm": 0.9106056690216064,
      "learning_rate": 8.679334916864608e-05,
      "loss": 0.2368,
      "step": 7150
    },
    {
      "epoch": 1.7102137767220902,
      "grad_norm": 2.190723419189453,
      "learning_rate": 8.600158353127474e-05,
      "loss": 0.1991,
      "step": 7200
    },
    {
      "epoch": 1.7220902612826603,
      "grad_norm": 2.1304259300231934,
      "learning_rate": 8.52098178939034e-05,
      "loss": 0.2328,
      "step": 7250
    },
    {
      "epoch": 1.7339667458432304,
      "grad_norm": 2.0825417041778564,
      "learning_rate": 8.441805225653206e-05,
      "loss": 0.2024,
      "step": 7300
    },
    {
      "epoch": 1.7458432304038005,
      "grad_norm": 0.4094597399234772,
      "learning_rate": 8.362628661916073e-05,
      "loss": 0.1984,
      "step": 7350
    },
    {
      "epoch": 1.7577197149643706,
      "grad_norm": 2.499983549118042,
      "learning_rate": 8.28345209817894e-05,
      "loss": 0.1765,
      "step": 7400
    },
    {
      "epoch": 1.7695961995249405,
      "grad_norm": 3.5041539669036865,
      "learning_rate": 8.204275534441806e-05,
      "loss": 0.2011,
      "step": 7450
    },
    {
      "epoch": 1.7814726840855108,
      "grad_norm": 2.151693820953369,
      "learning_rate": 8.125098970704671e-05,
      "loss": 0.2122,
      "step": 7500
    },
    {
      "epoch": 1.7933491686460807,
      "grad_norm": 3.658637762069702,
      "learning_rate": 8.045922406967538e-05,
      "loss": 0.2294,
      "step": 7550
    },
    {
      "epoch": 1.8052256532066508,
      "grad_norm": 1.9355175495147705,
      "learning_rate": 7.966745843230405e-05,
      "loss": 0.1584,
      "step": 7600
    },
    {
      "epoch": 1.817102137767221,
      "grad_norm": 1.2331641912460327,
      "learning_rate": 7.88756927949327e-05,
      "loss": 0.2167,
      "step": 7650
    },
    {
      "epoch": 1.8289786223277908,
      "grad_norm": 2.7249934673309326,
      "learning_rate": 7.808392715756137e-05,
      "loss": 0.2206,
      "step": 7700
    },
    {
      "epoch": 1.8408551068883612,
      "grad_norm": 0.15607717633247375,
      "learning_rate": 7.729216152019004e-05,
      "loss": 0.1958,
      "step": 7750
    },
    {
      "epoch": 1.852731591448931,
      "grad_norm": 0.8223128318786621,
      "learning_rate": 7.650039588281869e-05,
      "loss": 0.1756,
      "step": 7800
    },
    {
      "epoch": 1.8646080760095012,
      "grad_norm": 2.9714980125427246,
      "learning_rate": 7.570863024544736e-05,
      "loss": 0.2135,
      "step": 7850
    },
    {
      "epoch": 1.8764845605700713,
      "grad_norm": 1.859053611755371,
      "learning_rate": 7.491686460807601e-05,
      "loss": 0.189,
      "step": 7900
    },
    {
      "epoch": 1.8883610451306412,
      "grad_norm": 1.7552367448806763,
      "learning_rate": 7.412509897070468e-05,
      "loss": 0.2225,
      "step": 7950
    },
    {
      "epoch": 1.9002375296912115,
      "grad_norm": 4.097362518310547,
      "learning_rate": 7.333333333333333e-05,
      "loss": 0.1738,
      "step": 8000
    },
    {
      "epoch": 1.9121140142517814,
      "grad_norm": 1.64601731300354,
      "learning_rate": 7.2541567695962e-05,
      "loss": 0.2005,
      "step": 8050
    },
    {
      "epoch": 1.9239904988123515,
      "grad_norm": 1.992457389831543,
      "learning_rate": 7.174980205859067e-05,
      "loss": 0.2038,
      "step": 8100
    },
    {
      "epoch": 1.9358669833729216,
      "grad_norm": 0.4499535858631134,
      "learning_rate": 7.095803642121932e-05,
      "loss": 0.1776,
      "step": 8150
    },
    {
      "epoch": 1.9477434679334917,
      "grad_norm": 3.0309481620788574,
      "learning_rate": 7.016627078384799e-05,
      "loss": 0.1886,
      "step": 8200
    },
    {
      "epoch": 1.9596199524940618,
      "grad_norm": 0.5589901804924011,
      "learning_rate": 6.937450514647664e-05,
      "loss": 0.2117,
      "step": 8250
    },
    {
      "epoch": 1.9714964370546317,
      "grad_norm": 0.7805600762367249,
      "learning_rate": 6.858273950910531e-05,
      "loss": 0.198,
      "step": 8300
    },
    {
      "epoch": 1.9833729216152018,
      "grad_norm": 3.6170284748077393,
      "learning_rate": 6.779097387173396e-05,
      "loss": 0.192,
      "step": 8350
    },
    {
      "epoch": 1.995249406175772,
      "grad_norm": 2.048706293106079,
      "learning_rate": 6.699920823436263e-05,
      "loss": 0.1741,
      "step": 8400
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8956422018348624,
      "eval_loss": 0.30484503507614136,
      "eval_runtime": 24.9017,
      "eval_samples_per_second": 35.018,
      "eval_steps_per_second": 2.209,
      "step": 8420
    },
    {
      "epoch": 2.007125890736342,
      "grad_norm": 1.7901060581207275,
      "learning_rate": 6.62074425969913e-05,
      "loss": 0.1909,
      "step": 8450
    },
    {
      "epoch": 2.019002375296912,
      "grad_norm": 1.7426817417144775,
      "learning_rate": 6.541567695961995e-05,
      "loss": 0.1699,
      "step": 8500
    },
    {
      "epoch": 2.030878859857482,
      "grad_norm": 3.4511497020721436,
      "learning_rate": 6.462391132224862e-05,
      "loss": 0.1898,
      "step": 8550
    },
    {
      "epoch": 2.0427553444180524,
      "grad_norm": 4.287017822265625,
      "learning_rate": 6.383214568487727e-05,
      "loss": 0.1797,
      "step": 8600
    },
    {
      "epoch": 2.0546318289786223,
      "grad_norm": 3.237602710723877,
      "learning_rate": 6.304038004750594e-05,
      "loss": 0.159,
      "step": 8650
    },
    {
      "epoch": 2.0665083135391926,
      "grad_norm": 2.4431588649749756,
      "learning_rate": 6.22486144101346e-05,
      "loss": 0.1564,
      "step": 8700
    },
    {
      "epoch": 2.0783847980997625,
      "grad_norm": 2.599148750305176,
      "learning_rate": 6.145684877276327e-05,
      "loss": 0.1863,
      "step": 8750
    },
    {
      "epoch": 2.0902612826603324,
      "grad_norm": 2.0525519847869873,
      "learning_rate": 6.066508313539193e-05,
      "loss": 0.2304,
      "step": 8800
    },
    {
      "epoch": 2.1021377672209027,
      "grad_norm": 4.300695896148682,
      "learning_rate": 5.9873317498020585e-05,
      "loss": 0.1737,
      "step": 8850
    },
    {
      "epoch": 2.1140142517814726,
      "grad_norm": 1.3313157558441162,
      "learning_rate": 5.908155186064925e-05,
      "loss": 0.1644,
      "step": 8900
    },
    {
      "epoch": 2.125890736342043,
      "grad_norm": 2.6900947093963623,
      "learning_rate": 5.8289786223277906e-05,
      "loss": 0.1689,
      "step": 8950
    },
    {
      "epoch": 2.137767220902613,
      "grad_norm": 0.6023951172828674,
      "learning_rate": 5.749802058590657e-05,
      "loss": 0.1902,
      "step": 9000
    },
    {
      "epoch": 2.1496437054631827,
      "grad_norm": 1.9522008895874023,
      "learning_rate": 5.670625494853523e-05,
      "loss": 0.1571,
      "step": 9050
    },
    {
      "epoch": 2.161520190023753,
      "grad_norm": 2.3097288608551025,
      "learning_rate": 5.59144893111639e-05,
      "loss": 0.1698,
      "step": 9100
    },
    {
      "epoch": 2.173396674584323,
      "grad_norm": 0.8247942328453064,
      "learning_rate": 5.512272367379257e-05,
      "loss": 0.2247,
      "step": 9150
    },
    {
      "epoch": 2.1852731591448933,
      "grad_norm": 2.8510613441467285,
      "learning_rate": 5.433095803642122e-05,
      "loss": 0.1592,
      "step": 9200
    },
    {
      "epoch": 2.197149643705463,
      "grad_norm": 4.87104606628418,
      "learning_rate": 5.353919239904989e-05,
      "loss": 0.1745,
      "step": 9250
    },
    {
      "epoch": 2.209026128266033,
      "grad_norm": 2.88797926902771,
      "learning_rate": 5.274742676167854e-05,
      "loss": 0.2,
      "step": 9300
    },
    {
      "epoch": 2.2209026128266034,
      "grad_norm": 6.533809185028076,
      "learning_rate": 5.195566112430721e-05,
      "loss": 0.1642,
      "step": 9350
    },
    {
      "epoch": 2.2327790973871733,
      "grad_norm": 2.737422466278076,
      "learning_rate": 5.116389548693586e-05,
      "loss": 0.15,
      "step": 9400
    },
    {
      "epoch": 2.2446555819477436,
      "grad_norm": 1.2737860679626465,
      "learning_rate": 5.037212984956453e-05,
      "loss": 0.1842,
      "step": 9450
    },
    {
      "epoch": 2.2565320665083135,
      "grad_norm": 3.7405707836151123,
      "learning_rate": 4.958036421219319e-05,
      "loss": 0.1542,
      "step": 9500
    },
    {
      "epoch": 2.268408551068884,
      "grad_norm": 1.6163792610168457,
      "learning_rate": 4.878859857482185e-05,
      "loss": 0.1903,
      "step": 9550
    },
    {
      "epoch": 2.2802850356294537,
      "grad_norm": 1.3292460441589355,
      "learning_rate": 4.799683293745051e-05,
      "loss": 0.1624,
      "step": 9600
    },
    {
      "epoch": 2.2921615201900236,
      "grad_norm": 3.3660008907318115,
      "learning_rate": 4.720506730007918e-05,
      "loss": 0.1768,
      "step": 9650
    },
    {
      "epoch": 2.304038004750594,
      "grad_norm": 0.536722719669342,
      "learning_rate": 4.6413301662707845e-05,
      "loss": 0.1988,
      "step": 9700
    },
    {
      "epoch": 2.315914489311164,
      "grad_norm": 3.358182907104492,
      "learning_rate": 4.5621536025336506e-05,
      "loss": 0.1806,
      "step": 9750
    },
    {
      "epoch": 2.3277909738717337,
      "grad_norm": 2.822613477706909,
      "learning_rate": 4.4829770387965166e-05,
      "loss": 0.2092,
      "step": 9800
    },
    {
      "epoch": 2.339667458432304,
      "grad_norm": 3.2671115398406982,
      "learning_rate": 4.4038004750593827e-05,
      "loss": 0.2083,
      "step": 9850
    },
    {
      "epoch": 2.351543942992874,
      "grad_norm": 2.4580631256103516,
      "learning_rate": 4.324623911322249e-05,
      "loss": 0.1543,
      "step": 9900
    },
    {
      "epoch": 2.3634204275534443,
      "grad_norm": 2.129434108734131,
      "learning_rate": 4.245447347585115e-05,
      "loss": 0.1647,
      "step": 9950
    },
    {
      "epoch": 2.375296912114014,
      "grad_norm": 1.348813533782959,
      "learning_rate": 4.1662707838479814e-05,
      "loss": 0.1985,
      "step": 10000
    },
    {
      "epoch": 2.3871733966745845,
      "grad_norm": 1.4340161085128784,
      "learning_rate": 4.0870942201108475e-05,
      "loss": 0.2023,
      "step": 10050
    },
    {
      "epoch": 2.3990498812351544,
      "grad_norm": 1.399234414100647,
      "learning_rate": 4.0079176563737135e-05,
      "loss": 0.1741,
      "step": 10100
    },
    {
      "epoch": 2.4109263657957243,
      "grad_norm": 5.214761734008789,
      "learning_rate": 3.9287410926365796e-05,
      "loss": 0.1905,
      "step": 10150
    },
    {
      "epoch": 2.4228028503562946,
      "grad_norm": 4.45470666885376,
      "learning_rate": 3.8495645288994456e-05,
      "loss": 0.1854,
      "step": 10200
    },
    {
      "epoch": 2.4346793349168645,
      "grad_norm": 0.9922749996185303,
      "learning_rate": 3.7703879651623116e-05,
      "loss": 0.189,
      "step": 10250
    },
    {
      "epoch": 2.446555819477435,
      "grad_norm": 2.5793681144714355,
      "learning_rate": 3.6912114014251784e-05,
      "loss": 0.1749,
      "step": 10300
    },
    {
      "epoch": 2.4584323040380047,
      "grad_norm": 2.027801752090454,
      "learning_rate": 3.612034837688045e-05,
      "loss": 0.1341,
      "step": 10350
    },
    {
      "epoch": 2.470308788598575,
      "grad_norm": 3.1670658588409424,
      "learning_rate": 3.532858273950911e-05,
      "loss": 0.158,
      "step": 10400
    },
    {
      "epoch": 2.482185273159145,
      "grad_norm": 3.879822015762329,
      "learning_rate": 3.453681710213777e-05,
      "loss": 0.165,
      "step": 10450
    },
    {
      "epoch": 2.494061757719715,
      "grad_norm": 2.2640175819396973,
      "learning_rate": 3.374505146476643e-05,
      "loss": 0.1819,
      "step": 10500
    },
    {
      "epoch": 2.505938242280285,
      "grad_norm": 1.6768378019332886,
      "learning_rate": 3.295328582739509e-05,
      "loss": 0.1521,
      "step": 10550
    },
    {
      "epoch": 2.517814726840855,
      "grad_norm": 2.903294324874878,
      "learning_rate": 3.216152019002375e-05,
      "loss": 0.2026,
      "step": 10600
    },
    {
      "epoch": 2.529691211401425,
      "grad_norm": 2.4614760875701904,
      "learning_rate": 3.136975455265241e-05,
      "loss": 0.1702,
      "step": 10650
    },
    {
      "epoch": 2.5415676959619953,
      "grad_norm": 1.6397274732589722,
      "learning_rate": 3.057798891528108e-05,
      "loss": 0.205,
      "step": 10700
    },
    {
      "epoch": 2.553444180522565,
      "grad_norm": 1.2572932243347168,
      "learning_rate": 2.978622327790974e-05,
      "loss": 0.1669,
      "step": 10750
    },
    {
      "epoch": 2.5653206650831355,
      "grad_norm": 2.581010341644287,
      "learning_rate": 2.8994457640538404e-05,
      "loss": 0.1543,
      "step": 10800
    },
    {
      "epoch": 2.5771971496437054,
      "grad_norm": 1.2536230087280273,
      "learning_rate": 2.8202692003167065e-05,
      "loss": 0.1773,
      "step": 10850
    },
    {
      "epoch": 2.5890736342042757,
      "grad_norm": 1.4121898412704468,
      "learning_rate": 2.7410926365795725e-05,
      "loss": 0.17,
      "step": 10900
    },
    {
      "epoch": 2.6009501187648456,
      "grad_norm": 4.699690341949463,
      "learning_rate": 2.6619160728424385e-05,
      "loss": 0.1439,
      "step": 10950
    },
    {
      "epoch": 2.6128266033254155,
      "grad_norm": 4.874575614929199,
      "learning_rate": 2.5827395091053046e-05,
      "loss": 0.181,
      "step": 11000
    },
    {
      "epoch": 2.624703087885986,
      "grad_norm": 2.3160979747772217,
      "learning_rate": 2.5035629453681713e-05,
      "loss": 0.1857,
      "step": 11050
    },
    {
      "epoch": 2.6365795724465557,
      "grad_norm": 1.5747265815734863,
      "learning_rate": 2.4243863816310373e-05,
      "loss": 0.1906,
      "step": 11100
    },
    {
      "epoch": 2.648456057007126,
      "grad_norm": 2.2818899154663086,
      "learning_rate": 2.3452098178939037e-05,
      "loss": 0.196,
      "step": 11150
    },
    {
      "epoch": 2.660332541567696,
      "grad_norm": 2.9305410385131836,
      "learning_rate": 2.2660332541567697e-05,
      "loss": 0.1709,
      "step": 11200
    },
    {
      "epoch": 2.6722090261282663,
      "grad_norm": 1.2987949848175049,
      "learning_rate": 2.1868566904196358e-05,
      "loss": 0.1903,
      "step": 11250
    },
    {
      "epoch": 2.684085510688836,
      "grad_norm": 1.8800373077392578,
      "learning_rate": 2.107680126682502e-05,
      "loss": 0.2055,
      "step": 11300
    },
    {
      "epoch": 2.695961995249406,
      "grad_norm": 6.582643508911133,
      "learning_rate": 2.0285035629453682e-05,
      "loss": 0.1762,
      "step": 11350
    },
    {
      "epoch": 2.7078384798099764,
      "grad_norm": 3.2081356048583984,
      "learning_rate": 1.9493269992082346e-05,
      "loss": 0.1899,
      "step": 11400
    },
    {
      "epoch": 2.7197149643705463,
      "grad_norm": 6.8202924728393555,
      "learning_rate": 1.8701504354711006e-05,
      "loss": 0.1331,
      "step": 11450
    },
    {
      "epoch": 2.731591448931116,
      "grad_norm": 1.1484023332595825,
      "learning_rate": 1.790973871733967e-05,
      "loss": 0.1641,
      "step": 11500
    },
    {
      "epoch": 2.7434679334916865,
      "grad_norm": 4.4391374588012695,
      "learning_rate": 1.711797307996833e-05,
      "loss": 0.2157,
      "step": 11550
    },
    {
      "epoch": 2.7553444180522564,
      "grad_norm": 2.3417201042175293,
      "learning_rate": 1.632620744259699e-05,
      "loss": 0.2028,
      "step": 11600
    },
    {
      "epoch": 2.7672209026128267,
      "grad_norm": 3.4719295501708984,
      "learning_rate": 1.5534441805225654e-05,
      "loss": 0.1645,
      "step": 11650
    },
    {
      "epoch": 2.7790973871733966,
      "grad_norm": 2.6016738414764404,
      "learning_rate": 1.4742676167854317e-05,
      "loss": 0.1886,
      "step": 11700
    },
    {
      "epoch": 2.790973871733967,
      "grad_norm": 1.95682954788208,
      "learning_rate": 1.3950910530482977e-05,
      "loss": 0.1772,
      "step": 11750
    },
    {
      "epoch": 2.802850356294537,
      "grad_norm": 2.0410821437835693,
      "learning_rate": 1.3159144893111639e-05,
      "loss": 0.1412,
      "step": 11800
    },
    {
      "epoch": 2.8147268408551067,
      "grad_norm": 2.657273769378662,
      "learning_rate": 1.2367379255740301e-05,
      "loss": 0.1525,
      "step": 11850
    },
    {
      "epoch": 2.826603325415677,
      "grad_norm": 1.1119303703308105,
      "learning_rate": 1.1575613618368963e-05,
      "loss": 0.1736,
      "step": 11900
    },
    {
      "epoch": 2.838479809976247,
      "grad_norm": 3.7567195892333984,
      "learning_rate": 1.0783847980997625e-05,
      "loss": 0.1864,
      "step": 11950
    },
    {
      "epoch": 2.850356294536817,
      "grad_norm": 2.409827470779419,
      "learning_rate": 9.992082343626287e-06,
      "loss": 0.1797,
      "step": 12000
    },
    {
      "epoch": 2.862232779097387,
      "grad_norm": 1.8102818727493286,
      "learning_rate": 9.20031670625495e-06,
      "loss": 0.1728,
      "step": 12050
    },
    {
      "epoch": 2.8741092636579575,
      "grad_norm": 2.364795446395874,
      "learning_rate": 8.40855106888361e-06,
      "loss": 0.1454,
      "step": 12100
    },
    {
      "epoch": 2.8859857482185274,
      "grad_norm": 3.7525296211242676,
      "learning_rate": 7.616785431512273e-06,
      "loss": 0.1565,
      "step": 12150
    },
    {
      "epoch": 2.8978622327790973,
      "grad_norm": 5.295376777648926,
      "learning_rate": 6.825019794140934e-06,
      "loss": 0.1664,
      "step": 12200
    },
    {
      "epoch": 2.9097387173396676,
      "grad_norm": 3.627912759780884,
      "learning_rate": 6.033254156769597e-06,
      "loss": 0.1522,
      "step": 12250
    },
    {
      "epoch": 2.9216152019002375,
      "grad_norm": 1.5286592245101929,
      "learning_rate": 5.241488519398258e-06,
      "loss": 0.1543,
      "step": 12300
    },
    {
      "epoch": 2.9334916864608074,
      "grad_norm": 0.7988382577896118,
      "learning_rate": 4.44972288202692e-06,
      "loss": 0.1936,
      "step": 12350
    },
    {
      "epoch": 2.9453681710213777,
      "grad_norm": 5.027170658111572,
      "learning_rate": 3.6579572446555818e-06,
      "loss": 0.1678,
      "step": 12400
    },
    {
      "epoch": 2.9572446555819476,
      "grad_norm": 0.8011447191238403,
      "learning_rate": 2.8661916072842443e-06,
      "loss": 0.1346,
      "step": 12450
    },
    {
      "epoch": 2.969121140142518,
      "grad_norm": 4.090683460235596,
      "learning_rate": 2.074425969912906e-06,
      "loss": 0.1957,
      "step": 12500
    },
    {
      "epoch": 2.980997624703088,
      "grad_norm": 5.991174697875977,
      "learning_rate": 1.2826603325415678e-06,
      "loss": 0.1483,
      "step": 12550
    },
    {
      "epoch": 2.992874109263658,
      "grad_norm": 4.246505260467529,
      "learning_rate": 4.908946951702296e-07,
      "loss": 0.1823,
      "step": 12600
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.893348623853211,
      "eval_loss": 0.355972558259964,
      "eval_runtime": 24.8788,
      "eval_samples_per_second": 35.05,
      "eval_steps_per_second": 2.211,
      "step": 12630
    }
  ],
  "logging_steps": 50,
  "max_steps": 12630,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6828804290442240.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
